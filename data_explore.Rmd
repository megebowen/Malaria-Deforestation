---
title: "Hansen Data Exploration"
author: "Meghan Bowen"
date: "May 18, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load_packages}

library(tidyverse) 
library(raster) ##load and manipulate rasters
library(sf) ##load shapefiles
library(sp) ##spatial objects
library(spatstat) ##work with shapefiles
library(tmap) ##map rasters + shapefiles
library(rgdal) ##for projections and coord systems

library(gfcanalysis) ##package specifically for Hansen Forestry Data!
library(landscapetools) ##for landscape plotting/viz?
library(landscapemetrics) ##R equivalent to FRAGSTATS


```


```{r load_rasters}

##load rasters

lossyear <- raster("Hansen_GFC-2018-v1.6_lossyear_00N_080W.tif")

#plot(lossyear)


##what is projection? datum is WGS84
lossyear@crs
```


```{r load_shps}
##Muni_shp: legal Amazon boundary
##muni_2010: Brazil boundary?? not sure

##selecting "uf" (is this the municipal boundary?)" variables only for now
muni_shp <- read_sf(dsn = 'municipality_shapefiles', layer = "Amazon_Municipios") %>% 
  dplyr::select(uf)
#shp projection is also WGS84

plot(muni_shp)

##selecting pib (??) in municipios 2010
muni_2010 <- read_sf(dsn = 'municipality_shapefiles', layer = "municipios_2010") %>% 
  dplyr::select(pib)

```

```{r test_gfc}

##calc_gfc_tiles: figure out which tiles are needed based on AOI (shp file above)
##for this function, need to convert muni_shp from multipolygon (spat out by sf) to a spatialpolygon object

#convert to spatial polygons
spd <- sf::as_Spatial(st_geometry(muni_shp),
                      IDs = as.character(1:nrow(muni_shp)))

#muni_df <- muni_shp
#muni_df$geometry <- NULL
muni_df <- as.data.frame(muni_shp)

muni_spd <- sp::SpatialPolygonsDataFrame(spd, data = muni_df)

tiles <- calc_gfc_tiles(aoi = muni_spd)
tiles

##once find the 'tiles' above, can use 'download_tiles' to grab the files from Hansen online (double check the dataset is correct!)

##once grabbed all 'download_tiles' above, can use 'extract_gfc' to grab the "change" layers (treecover2000, loss, gain, lossyear, datamask)

##threshold_gfc after

##THEN can use gfc_stats

```



```{r compress}

##expand from 0.00025 x 0.00025 by a factor of 10 --> 0.0025 X 0.0025
lossyear_res <- aggregate(lossyear, fact = 10)

##reproject?
lossyear_wgs <- projectRaster(lossyear_res, proj4string(+init=epsg:4326 +proj=longlat +ellps=WGS84))

#plot both together
plot(lossyear_res)
plot(muni_shp,add=T)

#crop muni to kinda fit this tile?
muni_bounds <- as(extent(-75, -70, -10, -4), 'SpatialPolygons')
crs(muni_bounds) <- crs(muni_2010)

crop_muni <- sf::st_crop(muni_2010, muni_bounds)

#plot both together
plot(lossyear_res)
plot(crop_muni,add=T)
```

```{r extract_mask}

##interest raster by muni?
loss_clip <- crop(lossyear_res, extent(muni_shp))

## crop and mask
lossyear_res0 <- crop(lossyear_res, extent(crop_muni))
lossyear_crop <- mask(lossyear_res0, crop_muni)

## Check that it worked
plot(lossyear_crop)
plot(crop_muni, add=TRUE, lwd=2)

```



```{r tmap}

##test map 1: border of shapefile ontop of raster
test_map <- tm_shape(lossyear_res) +
  tm_raster(title = "Test Map") +
  tm_shape(muni_shp) +
  tm_borders(col = "darkgreen")
  
test_map

##test map 2: "extract by mask" (lossyear_crop)
##not working?
test_map2 <- tm_shape(lossyear_crop) +
  tm_raster(title = "Test Map") +
  tm_shape(muni_shp) +
  tm_borders(col = "darkgreen")
  
test_map2

```


```{r explore_landscapemetrics}

##show landscape with all patches/etc
show_landscape(lossyear_crop)

```


##General Spatial R Questions:

1. **How to get these huge tile rasters down?! even by fact = 10 it took 20 minutes to get it down. Do we even want to get it down?**
2. can't use tm_shape with a shapefile with multiple columns? need to filter it to one attribute?
3. need a landscape raster? ie with patches, habitat types ...? or will the lossyear be the "landscape" and the patches/areas are in %loss of forest???

- double check the projection (make sure it's ESPG: 4326)
- nine states in the legal amazon (wikipedia)
## Research Questions

1. Does 
2.
3.
