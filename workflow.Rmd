---
title: "Workflow"
author: "Meghan Bowen"
date: "7/2/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##NOTES

- this is the general workflow to run the 
- all rasters downloaded from the Hansen et al Global Forestry Chage dataset (2018, v1.6) https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.6.html
- shapefiles from Andy Macdonald, covering the entire legal Amazon Basin

##1. Load Packages

```{r load_packages}

library(tidyverse) #data cleaning and manipulation

library(raster) ##load and manipulate rasters
library(sf) ##load shapefiles
library(sp) ##additional functions for spatial objects
library(spatstat) ##work with shapefiles
library(rgdal) ##for projections and coord systems

library(gfcanalysis) ##package specifically for Hansen Forestry Data!
library(landscapemetrics) ##R equivalent to FRAGSTATS

```


##2. Load & clean shapefiles

```{r load_shps}

muni_shp <- read_sf(dsn = 'municipality_shapefiles', layer = "Amazon_Municipios")


##need to convert the municipality .shp to a "SpatialDataFrame" so gfcanalysis can grab the forestry tiles online
spd <- sf::as_Spatial(st_geometry(muni_shp),
                      IDs = as.character(1:nrow(muni_shp)))

muni_df <- as.data.frame(muni_shp)

muni_spd <- sp::SpatialPolygonsDataFrame(spd, data = muni_df)

```


```{r split_munis}

muni_split <- split(muni_shp, muni_shp$nome)

##function 2: get raster data for every polygon
total_poly <- length(muni_spd@polygons) #note this will only work for spd

```


##3. Load rasters & mosaic

```{r download_all}

#find the # of tiles needed. 
tiles <- calc_gfc_tiles(aoi = muni_spd)
tiles

##CODE FOR ALL TILES

#1. Download. NEED TO CHANGE THE OUTPUT FOLDER!!!!  

##download_tiles(tiles, output_folder = ".", dataset = "GFC-2018-v1.6",images = c("treecover2000", "lossyear", "gain"))

```

```{r mosaic_amazon}

#2. Mosaic all rasters together

#3. Extract fxn to extract tiles BY the muni_spd
##extract_muni <- extract(XXMOSAIC, muni_spd)

```


